{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries we're going to use.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import coalesce, col, lag, lit, round, sum, avg, radians, acos, sin, cos\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import findspark\n",
    "import gpxpy\n",
    "\n",
    "# This library helps to the Python session within the current Jupyter Kernel to find Spark in the container. \n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innitialize the Spark Session and get the SparkContext from it.\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "display(spark, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the struct for the DataFrame\n",
    "columns = StructType([\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('latitude', DoubleType(), True),\n",
    "    StructField('longitude', DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Opens and parse the GPS track stored within the *.gpx file.\n",
    "# After parsing the file, the geo point are then loaded into a Spark DataFrame.\n",
    "with open('/opt/etl/data/vehicles/motorcycle/AAA_11B/recovery.05-Mar-2022.1025.gpx') as fr:\n",
    "    gpx_parser = gpxpy.parse(fr)\n",
    "    raw_geo_df = spark.createDataFrame(map(\n",
    "        lambda p: (\n",
    "            p.time,\n",
    "            p.latitude,\n",
    "            p.longitude\n",
    "        ),\n",
    "        gpx_parser.tracks[0].segments[0].points\n",
    "    ), columns).orderBy(col('time'), asceding=True)\n",
    "\n",
    "raw_geo_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the haversine formula within a Spark UDF for retrieving the accurate distances between points.\n",
    "# Refer to https://en.wikipedia.org/wiki/Haversine_formula for deeper knowledge of this formula.\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Allows to calculate distance between two points using\n",
    "    Haversine, reference https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    distance = acos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lat1 - lat2))\n",
    "    # Radius of earth in kilometers is 6371.01\n",
    "\n",
    "    return distance * 6371.01\n",
    "\n",
    "# Created a Windows with 1 lag to apply it into the latitude, longitude and times.\n",
    "w = Window().partitionBy().orderBy(col('time'))\n",
    "\n",
    "# Applies the window into the DataFrame containing the Geo points.\n",
    "# This will shift by 1 position all the rows within the DataFrame.\n",
    "windowed_df = raw_geo_df.select(\n",
    "    '*',\n",
    "    lag(col('time')).over(w).alias('old_time'),\n",
    "    lag(col('latitude')).over(w).alias('old_latitude'),\n",
    "    lag(col('longitude')).over(w).alias('old_longitude')\n",
    ")\n",
    "\n",
    "# Adds the columns:\n",
    "# distance_km: float - distance between point P(n) and P(n-1)\n",
    "# time_elapsed_hours: long - time elapsed between meassurements T(n) and T(n-1)\n",
    "# speed_kmh: float - speed at a given time in km/h\n",
    "consolidated_df = windowed_df\\\n",
    "    .withColumn('distance_km', coalesce(haversine(col('old_latitude'), col('old_longitude'), col('latitude'), col('longitude')), lit(0)))\\\n",
    "    .withColumn('time_elapsed_hours', coalesce(col('time').cast('long') - col('old_time').cast('long'), lit(0)) / 3600)\\\n",
    "    .withColumn('speed_kmh', round(coalesce(col('distance_km') / col('time_elapsed_hours'), lit(0)), 2)).na.drop()\n",
    "\n",
    "consolidated_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Pandas DataFrame after deleting any NA found in the DataFrame\n",
    "pandas_df = consolidated_df.na.drop().toPandas().set_index('time')\n",
    "\n",
    "# Gets a summary of:\n",
    "# total_time_ride_hours: float - total time taken within the ride from start to end in hours.\n",
    "# total_distance_km: float - total distance rode from start to end.\n",
    "# avg_speed_ride_kmh: float - Average speed during the entire ride.\n",
    "# avg_measurement_interval_seconds: float - Average time between intervals in seconds.\n",
    "summary_df = consolidated_df.na.drop().select(\n",
    "    sum('time_elapsed_hours').alias('total_time_ride_hours'),\n",
    "    sum('distance_km').alias('total_distance_ride_km'),\n",
    "    avg('speed_kmh').alias('avg_speed_ride_kmh'),\n",
    "    (avg('time_elapsed_hours') * 3600).alias('avg_measurement_interval_seconds')\n",
    ")\n",
    "\n",
    "# Converts the summary DataFrame into a Python dictionary.\n",
    "summary = summary_df.rdd.map(lambda row: row.asDict()).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the speed during the entire ride.\n",
    "# There are several peaks that look abnormal in the diagram.\n",
    "# This could mean an error of the GPS tracking app while was recording the Geolocations.\n",
    "# This can be solved by normalizing the Time Series, cleaning the raw data, replacing the points by the average of P(n-1) and P(n+1),\n",
    "# removing those point or setting a range to ignore those gaps.\n",
    "pandas_df['speed_kmh'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example let's set a range between 0 and 120 km/h to ignore those gaps.\n",
    "\n",
    "pandas_df[pandas_df['speed_kmh'] > 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = consolidated_df.na.drop().filter(col('speed_kmh') <= 120).select(\n",
    "    sum('time_elapsed_hours').alias('total_time_ride_hours'),\n",
    "    sum('distance_km').alias('total_distance_ride_km'),\n",
    "    avg('speed_kmh').alias('avg_speed_ride_kmh'),\n",
    "    (avg('time_elapsed_hours') * 3600).alias('avg_measurement_interval_seconds')\n",
    ")\n",
    "\n",
    "summary = summary_df.rdd.map(lambda row: row.asDict()).collect()[0]\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df[pandas_df['speed_kmh'] <= 120]['speed_kmh'].plot()\n",
    "plt.axhline(y=summary['avg_speed_ride_kmh'], color='red')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fe1ca4872e8b6e4dc707ce4a5b465dfe0915700da83801a228a551a0ea6d9d4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
